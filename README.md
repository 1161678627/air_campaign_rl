# air_campaign_rl
**基于强化学习的游戏空战推演**

## Q-learning笔记
    以游戏为案例说明Q-learning算法的核心思想和应用
就我当前的理解Q-learning算法的核心就在于Q表的建立，Q表中记录了agent当前已经经历过的各种state
和这些state下每个action对应的reward，这就相当于一个攻略一样，agent在应用时只需要在Q表（攻略）
上查找当前state对应的reward最大的action去执行即可。

其实可以看作以流程化的数学模型，使得agent能够在各种陌生的环境中进行经验学习，然后利用Q-learning
的决策思想和算法更新策略，不断地优化自己学习到的经验，并把这些经验存储下（Q表-先验知识）以指导新轮
次的游戏拿到更好的成绩。所以只要Q表建立的足够大，完全可以穷举游戏中的所有可能场景，然后根据先验知
识做出每一步的最优策略。

### Q-learning算法有三个要点：
    1.如何建立Q表->agent以什么策略自动的在环境中探索
    2.如何更新Q表的reward->每一步的决策依赖于前面的许多步骤，如何将reward链式传递给前面的s,a
    3.如何应用Q表->以什么原则使用Q表中的state和action 定位的Q值 完成action选择

但显然Q表不能无限扩展，很多游戏的场景复杂度是呈指数级增长的，根本无法穷举，其次当Q表过大时，决策
查询复杂度非常大，程序的执行效率会很低，所以Q-learning只能解决简单场景下的问题，甚至我们可以使
用规则来完成这些场景下的决策。


## Deep Q-learning笔记
dqn是Q-learning和深度学习结合的产物，其出现主要是为了解决Q表不能无限拓展的问题。其实Q表的核心
作用就是选择某个state下最佳的action，从输入输出的角度看Q表完成的工作就是输入一个state，输出该
state对应的各个action的Q值，然后我们依据Q-learning的action选择策略，根据Q值选择当前需要执行
的动作。简单来说Q表的作用就是 _输出某个state下各个action的Q值，以供RL算法决策_ ，因此我们可以
用神经网络代替Q表来完成 _从state->各个action的Q值的_ 的输入输出。也就是说即利用了RL善于决策的
优点，又利用了神经网络强大的特征提取、学习能力。

### 神经网络
_神经网络本质就是一个函数近似器_，只要网络结构设计的合理，模型足够大，能够拟合任意的函数，我们用
他来拟合 Q表自然也是可行的，输入一定维度的state（可以是数组形式的输入，也可以输图片形式的输入）
（考虑到游戏 进程的序列性，可以是多输入也可以是单输出），输出一组动作对应的Q值，交由RL决策。

## 如何训练
    根据以上思路，训练用于RL的神经网络的训练数据需要如下格式，state作为特征，每个动作真实的Q值作为标签
在训练时，我们可以将reward看作Q值理解，我们首先开启一个游戏环境，初始时使用dqn模型根据当前输入的
obs随机的做出action决策给agent探索环境，当agent执行完动作后，得到next_obs, reward, done,
等信息，然后我们记录remember这组数据，继续探索环境。等到remember的数据到达一定量之后，开始将
这些保存的数据输入给model，然后model输出一个预测的reward_，然后我们根据真实的reward和reward_
之间的差距，开始使用loss进行模型的训练，期待model预测的reward_能够逼近真实的reward。这样即使
在一个陌生的state下，model也能近似输出每个action对应的reward以供RL决策。
